{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgLrIce2ZAaS",
        "outputId": "c01ea58e-7b95-4929-9e75-70a85bc0d133"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved top_A9_usage.csv\n",
            "Saved top_A2_usage.csv\n",
            "Saved top_A7_usage.csv\n",
            "Saved top_A4_usage.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (update 'file_path' with the actual file path)\n",
        "file_path = 'API.csv'  # Replace with your CSV file path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Count the occurrences of each API\n",
        "top_apis = data['API Code'].value_counts().head(4).index.tolist()\n",
        "\n",
        "# Filter and save each top API to a separate CSV file\n",
        "for api in top_apis:\n",
        "    api_data = data[data['API Code'] == api]\n",
        "    file_name = f\"top_{api}_usage.csv\"\n",
        "    api_data.to_csv(file_name, index=False)\n",
        "    print(f\"Saved {file_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas numpy tensorflow scikit-learn matplotlib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_noKNk5Zih0",
        "outputId": "94a523f9-ddb9-4ac7-e967-7094e35dad57"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.0)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "pEYvsYdrk_pa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oEyDpPc7uijl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CtJRjAbaxiNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the dataset (replace with your file path)\n",
        "data = pd.read_csv(\"top_A2_usage.csv\")  # Replace with actual file\n",
        "data['Time of Call'] = pd.to_datetime(data['Time of Call'],dayfirst=True)\n",
        "data.set_index('Time of Call', inplace=True)\n",
        "\n",
        "# Aggregate API calls by time (e.g., hourly)\n",
        "time_series = data.resample('H').count()['API Code']\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler()\n",
        "time_series_scaled = scaler.fit_transform(time_series.values.reshape(-1, 1))\n",
        "\n",
        "# Create supervised learning dataset\n",
        "def create_supervised(data, lag=24):\n",
        "    X, y = [], []\n",
        "    for i in range(lag, len(data)):\n",
        "        X.append(data[i-lag:i])\n",
        "        y.append(data[i])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "lag = 24  # Use the last 24 hours to predict the next step\n",
        "X, y = create_supervised(time_series_scaled, lag)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qd28NNjMujUl",
        "outputId": "0f09f9ad-dacb-451d-b792-b72b90c745c9"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-41-5e9ecd5ef73d>:14: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
            "  time_series = data.resample('H').count()['API Code']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "def build_lstm(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(50, activation='relu', input_shape=input_shape))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "aZ1CPu1OvMhh"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import GRU\n",
        "\n",
        "def build_gru(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(GRU(50, activation='relu', input_shape=input_shape))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "o1f11Vv5vQln"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_mlp(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, activation='relu', input_shape=(input_shape[0],)))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "kaTfptwzvUL3"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Conv1D, GlobalAveragePooling1D\n",
        "\n",
        "def build_fcn(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=input_shape))\n",
        "    model.add(GlobalAveragePooling1D())\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "ZXkK_8-ivXEG"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Add, BatchNormalization\n",
        "\n",
        "def build_resnet(input_shape):\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "    x = Conv1D(64, kernel_size=3, padding='same', activation='relu')(input_layer)\n",
        "    x = BatchNormalization()(x)\n",
        "    residual = x\n",
        "\n",
        "    for _ in range(3):  # Add 3 residual blocks\n",
        "        x = Conv1D(64, kernel_size=3, padding='same', activation='relu')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Add()([x, residual])\n",
        "        residual = x\n",
        "\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    output_layer = Dense(1)(x)\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "KTvM4fvmvZEr"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "6ZxqeIQnv7FC"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import sqrt\n",
        "\n",
        "def train_and_evaluate(model_builder, X_train, y_train, X_test, y_test, model_name):\n",
        "    model = model_builder(X_train.shape[1:])\n",
        "    history = model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0, validation_data=(X_test, y_test))\n",
        "    y_pred = model.predict(X_test)\n",
        "    rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
        "    print(f\"{model_name} RMSE: {rmse}\")\n",
        "    return model, rmse\n",
        "\n",
        "# Train and evaluate all models\n",
        "models = {\n",
        "    \"LSTM\": build_lstm,\n",
        "    \"GRU\": build_gru,\n",
        "    \"MLP\": build_mlp,\n",
        "    \"FCN\": build_fcn,\n",
        "    \"ResNet\": build_resnet,\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for name, builder in models.items():\n",
        "    model, rmse = train_and_evaluate(builder, X_train, y_train, X_test, y_test, name)\n",
        "    results[name] = rmse\n",
        "\n",
        "# Print results\n",
        "print(\"Model Performance:\")\n",
        "for model_name, rmse in sorted(results.items(), key=lambda x: x[1]):\n",
        "    print(f\"{model_name}: RMSE = {rmse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y75A_bz9vbLN",
        "outputId": "fcd51c48-940b-46ca-edfd-ed6420f65f18"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "LSTM RMSE: 0.19032063865857665\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "GRU RMSE: 0.19167080856463697\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "MLP RMSE: 0.21118306462561762\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "FCN RMSE: 0.19068726677733489\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "ResNet RMSE: 0.20700977265612594\n",
            "Model Performance:\n",
            "LSTM: RMSE = 0.19032063865857665\n",
            "FCN: RMSE = 0.19068726677733489\n",
            "GRU: RMSE = 0.19167080856463697\n",
            "ResNet: RMSE = 0.20700977265612594\n",
            "MLP: RMSE = 0.21118306462561762\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the best model\n",
        "best_model_name = min(results, key=results.get)\n",
        "print(f\"Best Model: {best_model_name}\")\n",
        "\n",
        "# Retrain the best model on the entire dataset\n",
        "best_model = models[best_model_name](X_train.shape[1:])\n",
        "best_model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0)\n",
        "\n",
        "# Start with the last observed input\n",
        "current_input = X_test[-1].reshape(1, -1, 1)\n",
        "\n",
        "# Define time horizons\n",
        "horizons = [1, 24, 24 * 7, 24 * 30]  # Next hour, day, week, year\n",
        "\n",
        "# Sequential prediction for specific horizons\n",
        "future_predictions = []\n",
        "for horizon in horizons:\n",
        "    for _ in range(horizon - len(future_predictions)):\n",
        "        prediction = best_model.predict(current_input, verbose=0)\n",
        "        future_predictions.append(prediction[0, 0])\n",
        "        current_input = np.append(current_input[0, 1:], prediction).reshape(1, -1, 1)\n",
        "\n",
        "\n",
        "\n",
        "future_predictions = scaler.inverse_transform(np.array(future_predictions).reshape(-1, 1))\n",
        "\n",
        "# Calculate cumulative totals\n",
        "next_hour = round(future_predictions[0][0])  # Single prediction for the next hour\n",
        "next_day = round(sum(future_predictions[:24, 0]))  # Total for the first 24 predictions (Day 1)\n",
        "next_week = round(sum(future_predictions[:24 * 7, 0]))  # Total for Days 1-7 (Week)\n",
        "next_month = round(sum(future_predictions[:24 * 30, 0]))  # Total for Days 1-30 (Month)\n",
        "\n",
        "# Print results\n",
        "print(f\"Next Hour Prediction (Calls): {next_hour}\")\n",
        "print(f\"Total Calls Next Day (Day 1, 24 hours): {next_day}\")\n",
        "print(f\"Total Calls Next Week (Days 1-7): {next_week}\")\n",
        "print(f\"Total Calls Next Month (Days 1-30): {next_month}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoFWAHB9wdWj",
        "outputId": "1c97cc9c-af94-4367-ea58-744c09c64c88"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model: LSTM\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next Hour Prediction (Calls): 1\n",
            "Total Calls Next Day (Day 1, 24 hours): 14\n",
            "Total Calls Next Week (Days 1-7): 101\n",
            "Total Calls Next Month (Days 1-30): 434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the dataset (replace with your file path)\n",
        "data = pd.read_csv(\"top_A4_usage.csv\")  # Replace with actual file\n",
        "data['Time of Call'] = pd.to_datetime(data['Time of Call'],dayfirst=True)\n",
        "data.set_index('Time of Call', inplace=True)\n",
        "\n",
        "# Aggregate API calls by time (e.g., hourly)\n",
        "time_series = data.resample('H').count()['API Code']\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler()\n",
        "time_series_scaled = scaler.fit_transform(time_series.values.reshape(-1, 1))\n",
        "\n",
        "# Create supervised learning dataset\n",
        "def create_supervised(data, lag=24):\n",
        "    X, y = [], []\n",
        "    for i in range(lag, len(data)):\n",
        "        X.append(data[i-lag:i])\n",
        "        y.append(data[i])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "lag = 24  # Use the last 24 hours to predict the next step\n",
        "X, y = create_supervised(time_series_scaled, lag)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-5q_K3uzWGu",
        "outputId": "76c1d0eb-01ef-4449-ff90-b8cfb23f6493"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-72-34265028b870>:15: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
            "  time_series = data.resample('H').count()['API Code']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import LSTM, Conv1D, GlobalAveragePooling1D, Dropout, concatenate\n",
        "\n",
        "def build_lstm_fcn(input_shape):\n",
        "    # LSTM branch\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "    lstm_out = LSTM(64)(input_layer)\n",
        "\n",
        "    # FCN branch\n",
        "    conv1 = Conv1D(128, kernel_size=8, activation='relu', padding='same')(input_layer)\n",
        "    conv2 = Conv1D(256, kernel_size=5, activation='relu', padding='same')(conv1)\n",
        "    conv3 = Conv1D(128, kernel_size=3, activation='relu', padding='same')(conv2)\n",
        "    gap = GlobalAveragePooling1D()(conv3)\n",
        "\n",
        "    # Merge branches\n",
        "    merged = concatenate([lstm_out, gap])\n",
        "    output_layer = Dense(1)(merged)\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "VjdYd92exjYS"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_gru_fcn(input_shape):\n",
        "    # GRU branch\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "    gru_out = GRU(64)(input_layer)\n",
        "\n",
        "    # FCN branch\n",
        "    conv1 = Conv1D(128, kernel_size=8, activation='relu', padding='same')(input_layer)\n",
        "    conv2 = Conv1D(256, kernel_size=5, activation='relu', padding='same')(conv1)\n",
        "    conv3 = Conv1D(128, kernel_size=3, activation='relu', padding='same')(conv2)\n",
        "    gap = GlobalAveragePooling1D()(conv3)\n",
        "\n",
        "    # Merge branches\n",
        "    merged = concatenate([gru_out, gap])\n",
        "    output_layer = Dense(1)(merged)\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "00BZGcIK7fKy"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyWavelets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kp-SwJpR8FjN",
        "outputId": "3754815d-d7e8-40d4-c38e-8e2a254d1369"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyWavelets\n",
            "  Downloading pywavelets-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.10/dist-packages (from PyWavelets) (1.26.4)\n",
            "Downloading pywavelets-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/4.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m144.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyWavelets\n",
            "Successfully installed PyWavelets-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pywt\n",
        "\n",
        "def wavelet_decomposition(data, wavelet='db1', level=3):\n",
        "    coeffs = pywt.wavedec(data, wavelet, level=level)\n",
        "    reconstructed = pywt.waverec(coeffs, wavelet)\n",
        "    return np.array(coeffs).T  # Transpose for model input\n",
        "\n",
        "# Example model\n",
        "def build_mwdn(input_shape):\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "    x = Conv1D(64, kernel_size=3, activation='relu', padding='same')(input_layer)\n",
        "    x = LSTM(64, return_sequences=True)(x)\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    output_layer = Dense(1)(x)\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "aj1CrJKE7fWz"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import LayerNormalization, Activation\n",
        "\n",
        "def build_tcn(input_shape, nb_filters=64, kernel_size=3, nb_stacks=1, dilation_rates=[1, 2, 4, 8]):\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "    x = input_layer\n",
        "\n",
        "    for _ in range(nb_stacks):\n",
        "        for dilation_rate in dilation_rates:\n",
        "            x = Conv1D(nb_filters, kernel_size, padding=\"causal\", dilation_rate=dilation_rate)(x)\n",
        "            x = LayerNormalization()(x)\n",
        "            x = Activation('relu')(x)\n",
        "\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    output_layer = Dense(1)(x)\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "NE-hskYO7zVW"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_mlstm_fcn(input_shape):\n",
        "    # LSTM branch\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "    lstm_out = LSTM(64, return_sequences=True)(input_layer)\n",
        "    lstm_out = LSTM(32)(lstm_out)\n",
        "\n",
        "    # FCN branch\n",
        "    conv1 = Conv1D(128, kernel_size=8, activation='relu', padding='same')(input_layer)\n",
        "    conv2 = Conv1D(256, kernel_size=5, activation='relu', padding='same')(conv1)\n",
        "    conv3 = Conv1D(128, kernel_size=3, activation='relu', padding='same')(conv2)\n",
        "    gap = GlobalAveragePooling1D()(conv3)\n",
        "\n",
        "    # Merge branches\n",
        "    merged = concatenate([lstm_out, gap])\n",
        "    output_layer = Dense(1)(merged)\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "i6uUXGd57fel"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import sqrt\n",
        "\n",
        "def train_and_evaluate(model_builder, X_train, y_train, X_test, y_test, model_name):\n",
        "    model = model_builder(X_train.shape[1:])\n",
        "    history = model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0, validation_data=(X_test, y_test))\n",
        "    y_pred = model.predict(X_test)\n",
        "    rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
        "    print(f\"{model_name} RMSE: {rmse}\")\n",
        "    return model, rmse\n",
        "\n",
        "# Train and evaluate all models\n",
        "models = {\n",
        "    \"LSTM_FCN\": build_lstm_fcn,\n",
        "    \"GRU-FCN\": build_gru_fcn,\n",
        "    \"mWDN\": build_mwdn,\n",
        "    \"TCN \": build_tcn,\n",
        "    \"MLSTM-FCN\": build_mlstm_fcn,\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for name, builder in models.items():\n",
        "    model, rmse = train_and_evaluate(builder, X_train, y_train, X_test, y_test, name)\n",
        "    results[name] = rmse\n",
        "\n",
        "# Print results\n",
        "print(\"Model Performance:\")\n",
        "for model_name, rmse in sorted(results.items(), key=lambda x: x[1]):\n",
        "    print(f\"{model_name}: RMSE = {rmse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDEhKmBz7fg8",
        "outputId": "2240db09-df1b-4413-af3b-cf1f1ce9bd7b"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
            "LSTM_FCN RMSE: 0.15139491339634664\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
            "GRU-FCN RMSE: 0.1538991454327839\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "mWDN RMSE: 0.1508137151764486\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
            "TCN  RMSE: 0.15982444656313743\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "MLSTM-FCN RMSE: 0.15527828173544408\n",
            "Model Performance:\n",
            "mWDN: RMSE = 0.1508137151764486\n",
            "LSTM_FCN: RMSE = 0.15139491339634664\n",
            "GRU-FCN: RMSE = 0.1538991454327839\n",
            "MLSTM-FCN: RMSE = 0.15527828173544408\n",
            "TCN : RMSE = 0.15982444656313743\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the best model\n",
        "best_model_name = min(results, key=results.get)\n",
        "print(f\"Best Model: {best_model_name}\")\n",
        "\n",
        "# Retrain the best model on the entire dataset\n",
        "best_model = models[best_model_name](X_train.shape[1:])\n",
        "best_model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0)\n",
        "\n",
        "# Start with the last observed input\n",
        "current_input = X_test[-1].reshape(1, -1, 1)\n",
        "\n",
        "# Define time horizons\n",
        "horizons = [1, 24, 24 * 7, 24 * 30]  # Next hour, day, week, year\n",
        "\n",
        "# Sequential prediction for specific horizons\n",
        "future_predictions = []\n",
        "for horizon in horizons:\n",
        "    for _ in range(horizon - len(future_predictions)):\n",
        "        prediction = best_model.predict(current_input, verbose=0)\n",
        "        future_predictions.append(prediction[0, 0])\n",
        "        current_input = np.append(current_input[0, 1:], prediction).reshape(1, -1, 1)\n",
        "\n",
        "\n",
        "\n",
        "future_predictions = scaler.inverse_transform(np.array(future_predictions).reshape(-1, 1))\n",
        "\n",
        "# Calculate cumulative totals\n",
        "next_hour = round(future_predictions[0][0])  # Single prediction for the next hour\n",
        "next_day = round(sum(future_predictions[:24, 0]))  # Total for the first 24 predictions (Day 1)\n",
        "next_week = round(sum(future_predictions[:24 * 7, 0]))  # Total for Days 1-7 (Week)\n",
        "next_month = round(sum(future_predictions[:24 * 30, 0]))  # Total for Days 1-30 (Month)\n",
        "\n",
        "# Print results\n",
        "print(f\"Next Hour Prediction (Calls): {next_hour}\")\n",
        "print(f\"Total Calls Next Day (Day 1, 24 hours): {next_day}\")\n",
        "print(f\"Total Calls Next Week (Days 1-7): {next_week}\")\n",
        "print(f\"Total Calls Next Month (Days 1-30): {next_month}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiV4U17H7fka",
        "outputId": "93482ab2-f1f1-4534-9f78-cb1a44c31f51"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model: mWDN\n",
            "Next Hour Prediction (Calls): 1\n",
            "Total Calls Next Day (Day 1, 24 hours): 17\n",
            "Total Calls Next Week (Days 1-7): 111\n",
            "Total Calls Next Month (Days 1-30): 472\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the dataset (replace with your file path)\n",
        "data = pd.read_csv(\"top_A7_usage.csv\")  # Replace with actual file\n",
        "data['Time of Call'] = pd.to_datetime(data['Time of Call'],dayfirst=True)\n",
        "data.set_index('Time of Call', inplace=True)\n",
        "\n",
        "# Aggregate API calls by time (e.g., hourly)\n",
        "time_series = data.resample('H').count()['API Code']\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler()\n",
        "time_series_scaled = scaler.fit_transform(time_series.values.reshape(-1, 1))\n",
        "\n",
        "# Create supervised learning dataset\n",
        "def create_supervised(data, lag=24):\n",
        "    X, y = [], []\n",
        "    for i in range(lag, len(data)):\n",
        "        X.append(data[i-lag:i])\n",
        "        y.append(data[i])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "lag = 24  # Use the last 24 hours to predict the next step\n",
        "X, y = create_supervised(time_series_scaled, lag)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqFD9Ga0_bED",
        "outputId": "38d56844-9d25-44d2-8521-95b04bf8c34e"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-83-6e7d9c243c4b>:15: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
            "  time_series = data.resample('H').count()['API Code']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def build_inceptiontime(input_shape):\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    # First branch: 1x1 convolution\n",
        "    branch1 = layers.Conv1D(32, 1, padding='same', activation='relu')(input_layer)\n",
        "\n",
        "    # Second branch: 3x3 convolution\n",
        "    branch2 = layers.Conv1D(32, 3, padding='same', activation='relu')(input_layer)\n",
        "\n",
        "    # Third branch: 5x5 convolution\n",
        "    branch3 = layers.Conv1D(32, 5, padding='same', activation='relu')(input_layer)\n",
        "\n",
        "    # Concatenate all branches\n",
        "    concatenated = layers.concatenate([branch1, branch2, branch3])\n",
        "\n",
        "    # Global average pooling and dense layer\n",
        "    x = layers.GlobalAveragePooling1D()(concatenated)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    output = layers.Dense(1)(x)  # Change to 'Dense(num_classes, activation=\"softmax\")' for classification\n",
        "\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "85ud20Yn_e5Q"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uPzKeiOI_fGa"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_xceptiontime(input_shape):\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    # Xception block\n",
        "    x = layers.Conv1D(32, 3, padding='same', activation='relu')(input_layer)\n",
        "    x = layers.SeparableConv1D(64, 3, padding='same', activation='relu')(x)\n",
        "    x = layers.SeparableConv1D(128, 3, padding='same', activation='relu')(x)\n",
        "\n",
        "    # Global average pooling and dense layer\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    output = layers.Dense(1)(x)  # For regression, change for classification\n",
        "\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "SJReAGLF_fZF"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_rescnn(input_shape):\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    # First convolutional block with residual connections\n",
        "    x = layers.Conv1D(32, 3, padding='same', activation='relu')(input_layer)\n",
        "    x = layers.Conv1D(32, 3, padding='same', activation='relu')(x)\n",
        "    residual = layers.Conv1D(32, 1, padding='same')(input_layer)  # Residual connection\n",
        "    x = layers.add([x, residual])\n",
        "\n",
        "    # Second convolutional block\n",
        "    x = layers.Conv1D(64, 3, padding='same', activation='relu')(x)\n",
        "    x = layers.Conv1D(64, 3, padding='same', activation='relu')(x)\n",
        "    residual = layers.Conv1D(64, 1, padding='same')(x)  # Residual connection\n",
        "    x = layers.add([x, residual])\n",
        "\n",
        "    # Global average pooling and dense layer\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    output = layers.Dense(1)(x)  # For regression, change for classification\n",
        "\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "oMcBhQtK_oKO"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def build_xcm(input_shape):\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "    x = layers.Conv1D(64, 3, activation='relu', padding='same')(input_layer)\n",
        "    x = layers.Conv1D(128, 3, activation='relu', padding='same')(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    output = layers.Dense(1)(x)\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "veyCFoVu_oMN"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def build_tst(input_shape):\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "    x = layers.MultiHeadAttention(num_heads=8, key_dim=64)(input_layer, input_layer)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    output = layers.Dense(1)(x)\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "idipe39ZYQ-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import sqrt\n",
        "\n",
        "def train_and_evaluate(model_builder, X_train, y_train, X_test, y_test, model_name):\n",
        "    model = model_builder(X_train.shape[1:])\n",
        "    history = model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0, validation_data=(X_test, y_test))\n",
        "    y_pred = model.predict(X_test)\n",
        "    rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
        "    print(f\"{model_name} RMSE: {rmse}\")\n",
        "    return model, rmse\n",
        "\n",
        "# Train and evaluate all models\n",
        "models = {\n",
        "    \"InceptionTime\": build_inceptiontime,\n",
        "    \"XceptionTime\": build_xceptiontime,\n",
        "    \"ResCNN\": build_rescnn,\n",
        "    \"TST\": build_tst,\n",
        "    \"XCM\": build_xcm,\n",
        "\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for name, builder in models.items():\n",
        "    model, rmse = train_and_evaluate(builder, X_train, y_train, X_test, y_test, name)\n",
        "    results[name] = rmse\n",
        "\n",
        "# Print results\n",
        "print(\"Model Performance:\")\n",
        "for model_name, rmse in sorted(results.items(), key=lambda x: x[1]):\n",
        "    print(f\"{model_name}: RMSE = {rmse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcjPncKL_oPp",
        "outputId": "97977218-0821-46b7-ec57-38c9c152df8f"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
            "InceptionTime RMSE: 0.1583996205144647\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "XceptionTime RMSE: 0.1581217418184169\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "ResCNN RMSE: 0.15848909298692937\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "XCM RMSE: 0.1581608162193207\n",
            "Model Performance:\n",
            "XceptionTime: RMSE = 0.1581217418184169\n",
            "XCM: RMSE = 0.1581608162193207\n",
            "InceptionTime: RMSE = 0.1583996205144647\n",
            "ResCNN: RMSE = 0.15848909298692937\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the best model\n",
        "best_model_name = min(results, key=results.get)\n",
        "print(f\"Best Model: {best_model_name}\")\n",
        "\n",
        "# Retrain the best model on the entire dataset\n",
        "best_model = models[best_model_name](X_train.shape[1:])\n",
        "best_model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0)\n",
        "\n",
        "# Start with the last observed input\n",
        "current_input = X_test[-1].reshape(1, -1, 1)\n",
        "\n",
        "# Define time horizons\n",
        "horizons = [1, 24, 24 * 7, 24 * 30]  # Next hour, day, week, year\n",
        "\n",
        "# Sequential prediction for specific horizons\n",
        "future_predictions = []\n",
        "for horizon in horizons:\n",
        "    for _ in range(horizon - len(future_predictions)):\n",
        "        prediction = best_model.predict(current_input, verbose=0)\n",
        "        future_predictions.append(prediction[0, 0])\n",
        "        current_input = np.append(current_input[0, 1:], prediction).reshape(1, -1, 1)\n",
        "\n",
        "\n",
        "\n",
        "future_predictions = scaler.inverse_transform(np.array(future_predictions).reshape(-1, 1))\n",
        "\n",
        "# Calculate cumulative totals\n",
        "next_hour = round(future_predictions[0][0])  # Single prediction for the next hour\n",
        "next_day = round(sum(future_predictions[:24, 0]))  # Total for the first 24 predictions (Day 1)\n",
        "next_week = round(sum(future_predictions[:24 * 7, 0]))  # Total for Days 1-7 (Week)\n",
        "next_month = round(sum(future_predictions[:24 * 30, 0]))  # Total for Days 1-30 (Month)\n",
        "\n",
        "# Print results\n",
        "print(f\"Next Hour Prediction (Calls): {next_hour}\")\n",
        "print(f\"Total Calls Next Day (Day 1, 24 hours): {next_day}\")\n",
        "print(f\"Total Calls Next Week (Days 1-7): {next_week}\")\n",
        "print(f\"Total Calls Next Month (Days 1-30): {next_month}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9PwTSOnAxnn",
        "outputId": "6010533e-c414-4014-b8a2-9f35649b6ae1"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model: XceptionTime\n",
            "Next Hour Prediction (Calls): 1\n",
            "Total Calls Next Day (Day 1, 24 hours): 17\n",
            "Total Calls Next Week (Days 1-7): 120\n",
            "Total Calls Next Month (Days 1-30): 512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the dataset (replace with your file path)\n",
        "data = pd.read_csv(\"top_A9_usage.csv\")  # Replace with actual file\n",
        "data['Time of Call'] = pd.to_datetime(data['Time of Call'],dayfirst=True)\n",
        "data.set_index('Time of Call', inplace=True)\n",
        "\n",
        "# Aggregate API calls by time (e.g., hourly)\n",
        "time_series = data.resample('H').count()['API Code']\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler()\n",
        "time_series_scaled = scaler.fit_transform(time_series.values.reshape(-1, 1))\n",
        "\n",
        "# Create supervised learning dataset\n",
        "def create_supervised(data, lag=24):\n",
        "    X, y = [], []\n",
        "    for i in range(lag, len(data)):\n",
        "        X.append(data[i-lag:i])\n",
        "        y.append(data[i])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "lag = 24  # Use the last 24 hours to predict the next step\n",
        "X, y = create_supervised(time_series_scaled, lag)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMt44yDCJHIN",
        "outputId": "0c5948dd-b278-46f3-b98d-101f8b782259"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-104-037b1aba7044>:15: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
            "  time_series = data.resample('H').count()['API Code']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def build_tabtransformer(input_shape):\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "    x = layers.MultiHeadAttention(num_heads=8, key_dim=64)(input_layer, input_layer)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    output = layers.Dense(1)(x)\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "m7n4WWJcJMa1"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def build_tsit(input_shape):\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "    x = layers.Conv1D(64, 3, activation='relu', padding='same')(input_layer)\n",
        "    x = layers.LayerNormalization()(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    output = layers.Dense(1)(x)\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "X1tqtydOJMcl"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def build_gmlp(input_shape):\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "    x = layers.Dense(128, activation='relu')(input_layer)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    output = layers.Dense(1)(x)\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "gq8olEkoJMjr"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def build_tsperceiver(input_shape):\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "    x = layers.MultiHeadAttention(num_heads=8, key_dim=64)(input_layer, input_layer)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    output = layers.Dense(1)(x)\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "IKg_zbs6JMnJ"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_gated_tabtransformer(input_shape):\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "    x = layers.MultiHeadAttention(num_heads=8, key_dim=64)(input_layer, input_layer)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    output = layers.Dense(1)(x)\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "yS8Q9MSbTaVp"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def build_tssequencerplus(input_shape):\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "    x = layers.LSTM(128, return_sequences=True)(input_layer)\n",
        "    x = layers.LSTM(64)(x)\n",
        "    output = layers.Dense(1)(x)\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "g6jY1kX5JnsM"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_patchtst(input_shape):\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "    x = layers.Conv1D(64, 3, activation='relu', padding='same')(input_layer)\n",
        "    x = layers.MultiHeadAttention(num_heads=8, key_dim=64)(x, x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    output = layers.Dense(1)(x)\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "wG-CdmMsOBXc"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import sqrt\n",
        "\n",
        "def train_and_evaluate(model_builder, X_train, y_train, X_test, y_test, model_name):\n",
        "    model = model_builder(X_train.shape[1:])\n",
        "    history = model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0, validation_data=(X_test, y_test))\n",
        "    y_pred = model.predict(X_test)\n",
        "    rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
        "    print(f\"{model_name} RMSE: {rmse}\")\n",
        "    return model, rmse\n",
        "\n",
        "# Train and evaluate all models\n",
        "#\n",
        "\n",
        "\n",
        "models = {\n",
        "\n",
        "\n",
        "     \"PatchTST\": build_patchtst,\n",
        "    \"TabTransformer\": build_tabtransformer,\n",
        "    \"TSPerceiver\": build_tsperceiver,\n",
        "    \"GatedTabTransformer\": build_gated_tabtransformer,\n",
        "    \"TSSequencerPlus\": build_tssequencerplus\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for name, builder in models.items():\n",
        "    model, rmse = train_and_evaluate(builder, X_train, y_train, X_test, y_test, name)\n",
        "    results[name] = rmse\n",
        "\n",
        "# Print results\n",
        "print(\"Model Performance:\")\n",
        "for model_name, rmse in sorted(results.items(), key=lambda x: x[1]):\n",
        "    print(f\"{model_name}: RMSE = {rmse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6Ttc_5lJ0wA",
        "outputId": "cbcf9d92-4ddd-41d9-c0dc-d856339f31bd"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "PatchTST RMSE: 0.15831527391998212\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "TabTransformer RMSE: 0.16121663081541726\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "TSPerceiver RMSE: 0.1585658798221713\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
            "GatedTabTransformer RMSE: 0.15896102280398955\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
            "TSSequencerPlus RMSE: 0.15837739809113877\n",
            "Model Performance:\n",
            "PatchTST: RMSE = 0.15831527391998212\n",
            "TSSequencerPlus: RMSE = 0.15837739809113877\n",
            "TSPerceiver: RMSE = 0.1585658798221713\n",
            "GatedTabTransformer: RMSE = 0.15896102280398955\n",
            "TabTransformer: RMSE = 0.16121663081541726\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the best model\n",
        "best_model_name = min(results, key=results.get)\n",
        "print(f\"Best Model: {best_model_name}\")\n",
        "\n",
        "# Retrain the best model on the entire dataset\n",
        "best_model = models[best_model_name](X_train.shape[1:])\n",
        "best_model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0)\n",
        "\n",
        "# Start with the last observed input\n",
        "current_input = X_test[-1].reshape(1, -1, 1)\n",
        "\n",
        "# Define time horizons\n",
        "horizons = [1, 24, 24 * 7, 24 * 30]  # Next hour, day, week, year\n",
        "\n",
        "# Sequential prediction for specific horizons\n",
        "future_predictions = []\n",
        "for horizon in horizons:\n",
        "    for _ in range(horizon - len(future_predictions)):\n",
        "        prediction = best_model.predict(current_input, verbose=0)\n",
        "        future_predictions.append(prediction[0, 0])\n",
        "        current_input = np.append(current_input[0, 1:], prediction).reshape(1, -1, 1)\n",
        "\n",
        "\n",
        "\n",
        "future_predictions = scaler.inverse_transform(np.array(future_predictions).reshape(-1, 1))\n",
        "\n",
        "# Calculate cumulative totals\n",
        "next_hour = round(future_predictions[0][0])  # Single prediction for the next hour\n",
        "next_day = round(sum(future_predictions[:24, 0]))  # Total for the first 24 predictions (Day 1)\n",
        "next_week = round(sum(future_predictions[:24 * 7, 0]))  # Total for Days 1-7 (Week)\n",
        "next_month = round(sum(future_predictions[:24 * 30, 0]))  # Total for Days 1-30 (Month)\n",
        "\n",
        "# Print results\n",
        "print(f\"Next Hour Prediction (Calls): {next_hour}\")\n",
        "print(f\"Total Calls Next Day (Day 1, 24 hours): {next_day}\")\n",
        "print(f\"Total Calls Next Week (Days 1-7): {next_week}\")\n",
        "print(f\"Total Calls Next Month (Days 1-30): {next_month}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoSoHQuAOCUa",
        "outputId": "989652d3-5d12-43f5-9441-e7080544aa76"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model: PatchTST\n",
            "Next Hour Prediction (Calls): 1\n",
            "Total Calls Next Day (Day 1, 24 hours): 16\n",
            "Total Calls Next Week (Days 1-7): 108\n",
            "Total Calls Next Month (Days 1-30): 464\n"
          ]
        }
      ]
    }
  ]
}